{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarmanishjha/pgm-prjoject_A18/blob/master/DC_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "J0hITJKAgb5f",
        "colab_type": "code",
        "outputId": "d1a1109e-ee0f-44a0-db5f-57a584b7d8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "!pip install imageio  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57e34000 @  0x7f69cee3f2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Collecting imageio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (5.3.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "Successfully installed imageio-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7pmQU4sLK0Gw",
        "colab_type": "code",
        "outputId": "601dedcf-45ef-4639-9b77-3703ca102398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "added_path = 'drive/My Drive/results/MNIST_GAN_results/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yLhGMMF0gWAT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "p8Ty_Kb8gVSF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import imageio\n",
        "from torch.autograd import Variable\n",
        "import itertools\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YoCgCVQKgJfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data set"
      ]
    },
    {
      "metadata": {
        "id": "mClClq_SgI9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "\n",
        "train_MNIST = True\n",
        "train_CIFAR_10 = False\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "if train_MNIST:\n",
        "  train_data = datasets.MNIST('../data', train=True, \n",
        "                            download=True, transform=transform)\n",
        "  \n",
        "  n_channels = 1 \n",
        "  # test_data = datasets.MNIST('../data', train=False, transform=transform)\n",
        "  \n",
        "elif train_CIFAR_10:\n",
        "  # CIFAR 10 Data set !!\n",
        "  train_data = datasets.CIFAR10(root='./data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "  \n",
        "  n_channels = 3 \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDOimlIHlVVi",
        "colab_type": "code",
        "outputId": "224ac84f-185c-407a-fb75-1909440c87aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "cell_type": "code",
      "source": [
        "# This parameter influences optimization\n",
        "batch_size = 100\n",
        "# This is just for evaluation, we want is as big as the GPU can support\n",
        "batch_size_eval = 512\n",
        "\n",
        "# CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "    print('using cuda !')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "n_data_total = len(train_data)\n",
        "n_valid = 0\n",
        "n_train_images = n_data_total - n_valid\n",
        "print('n_data_total =', n_data_total, '; n_train_images =', \n",
        "       n_train_images, '; n_valid =', n_valid)\n",
        "\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[n_valid:]),\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "#valid_loader = DataLoader(\n",
        "#    train_data,\n",
        "#    batch_size=batch_size_eval,\n",
        "#    sampler=SubsetRandomSampler(indices[:n_valid]),\n",
        "#    #num_workers=1,\n",
        "#    pin_memory=use_cuda,\n",
        "#)\n",
        "\n",
        "\n",
        "# visualize and understand the data\n",
        "for inputs, targets in train_loader:\n",
        "    print(\"This is the shape of one batch:\", inputs.shape)\n",
        "    img = inputs[0, 0]\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    plt.imshow(img, cmap='Greys_r')\n",
        "    break    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cuda !\n",
            "n_data_total = 60000 ; n_train_images = 60000 ; n_valid = 0\n",
            "This is the shape of one batch: torch.Size([100, 1, 64, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJRGRAQZLIiIDDJZERAb+Hy8cSidl/sz/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8f3aa1e780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wNv4WjYnZm8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Choose if you want Wasserstein GAN or Vanilla GAN"
      ]
    },
    {
      "metadata": {
        "id": "-FpGQ9GXZlPn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_wgan = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XlFpenmYl46M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Samplers"
      ]
    },
    {
      "metadata": {
        "id": "dT_u5EKFl4MM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gaussian_noise_sampler(mu, sigma, device):\n",
        "    \"\"\" Gaussian input to the generator\"\"\"\n",
        "    return lambda m, n: torch.Tensor(np.random.normal(mu, sigma, (m, n))).to(device)\n",
        "\n",
        "\n",
        "def uniform_noise_sampler(a, b, device):\n",
        "    \"\"\" Uniform input to the generator of a m x n tensor\"\"\"\n",
        "    return lambda m, n: (b - a) * torch.rand(m, n, 1, 1).to(device) + a\n",
        "  \n",
        "def plot_a_gen_sample(gen_sample):\n",
        "    img = gen_sample.view(image_size, image_size)\n",
        "    plt.imshow(img, cmap='Greys_r')  \n",
        "    plt.show()\n",
        "\n",
        "# We feed in the generator a noise input   \n",
        "# Uniform sampler on [-1, 1]\n",
        "g_noise_input_generator = uniform_noise_sampler(-1, 1, device)\n",
        "# Input noise of size 100\n",
        "dim_gen_input = 100 \n",
        "\n",
        "fixed_noise = g_noise_input_generator(5 * 5, dim_gen_input)\n",
        "\n",
        "def show_result(model_G, dim_gen_input, num_epoch,\n",
        "                show = False, save = False, path = 'result.png', isFix=False):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "      random_noise = g_noise_input_generator(5 * 5, dim_gen_input)\n",
        "\n",
        "      if isFix:\n",
        "          test_images = model_G(fixed_noise)\n",
        "      else:\n",
        "          test_images = model_G(random_noise)\n",
        "\n",
        "      size_figure_grid = 5\n",
        "      fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "      for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "          ax[i, j].get_xaxis().set_visible(False)\n",
        "          ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "      for k in range(5*5):\n",
        "          i = k // 5\n",
        "          j = k % 5\n",
        "          ax[i, j].cla()\n",
        "          ax[i, j].imshow(test_images[k, :].cpu().data.view(image_size, image_size).numpy(), cmap='gray')\n",
        "\n",
        "      label = 'Epoch {0}'.format(num_epoch)\n",
        "      fig.text(0.5, 0.04, label, ha='center')\n",
        "      plt.savefig(path)\n",
        "\n",
        "      if show:\n",
        "          plt.show()\n",
        "      else:\n",
        "          plt.close()\n",
        "        \n",
        "    \n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()   \n",
        "  \n",
        "print('ok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XL0d-12qftha",
        "colab_type": "code",
        "outputId": "8d3ffb9e-9fc6-4f57-b9a8-5f5a3db298ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Standard slope of the leak of LeakyReLU is 0.01, 0.2 recommended by DCGAN paper\n",
        "# but 0.2 seems to work very badly on our 2d data sets\n",
        "slope_lrelu = 0.2\n",
        "# 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
        "d_steps = 1\n",
        "g_steps = 1\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  \n",
        "    def __init__(self, dim_input):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.ConvTranspose2d(dim_input, 1024, 4, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(1024)\n",
        "        # state size. (1024) x 4 x 4\n",
        "        self.conv2 = nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False)\n",
        "        self.bn2 =nn.BatchNorm2d(512)\n",
        "        # state size. (512) x 8 x 8\n",
        "        self.conv3 = nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        # state size. (256) x 16 x 16\n",
        "        self.conv4 = nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        # state size. (128) x 32 x 32\n",
        "        self.conv5 = nn.ConvTranspose2d(128, n_channels, 4, 2, 1, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "        # state size. (n_channels) x 64 x 64\n",
        "     \n",
        "\n",
        "    def forward(self, x):\n",
        "        # From https://github.com/soumith/ganhacks,\n",
        "        # use LeakyReLU to avoid sparse gradients\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.tanh(self.conv5(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "dim_images = int(image_size ** 2)\n",
        "\n",
        "model_G = Generator(dim_gen_input).to(device)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "         # input is (n_channels) x 64 x 64\n",
        "        self.conv1 = nn.Conv2d(n_channels, 128, 4, 2, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        # state size. (128) x 32 x 32\n",
        "        self.conv2 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
        "        self.bn2 =nn.BatchNorm2d(256)\n",
        "        # state size. (256) x 16 x 16\n",
        "        self.conv3 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(512)\n",
        "        # state size. (512) x 8 x 8\n",
        "        self.conv4 = nn.Conv2d(512, 1024, 4, 2, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(1024)\n",
        "        # state size. (1024) x 4 x 4\n",
        "        self.conv5 = nn.Conv2d(1024, n_channels, 4, 1, 0, bias=False)\n",
        "        # state size. (n_channels) x 64 x 64\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=slope_lrelu, inplace=True)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=slope_lrelu, inplace=True)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=slope_lrelu, inplace=True)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), negative_slope=slope_lrelu, inplace=True)\n",
        "     \n",
        "        # Training a standard binary classifier with a\n",
        "        # sigmoid output (--> using Binary Cross Entropy Loss\n",
        "        return self.sigmoid(self.conv5(x))\n",
        "\n",
        "\n",
        "# Single dimension for 'real' vs. 'fake'\n",
        "# We can change this if we want to use gans that also classify\n",
        "d_output_size = 1\n",
        "\n",
        "model_D = Discriminator().to(device)\n",
        "\n",
        "\n",
        "# Use same learning rates  for generator + discriminator\n",
        "# Learning rates recommended by the DCGAN paper for Adam\n",
        "learning_rate = 2e-4\n",
        "# From https://github.com/soumith/ganhacks, use Adam Optimizer\n",
        "d_optimizer = optim.Adam(model_D.parameters(), lr=learning_rate)\n",
        "g_optimizer = optim.Adam(model_G.parameters(), lr=learning_rate)\n",
        "# use binary cross entropy loss function\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "arr_gradient_D = []\n",
        "arr_gradient_G = []\n",
        "arr_means = []\n",
        "arr_std_dev = []\n",
        "# set to true or false to plot the mean and std_dev\n",
        "plot_means_and_var = False\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# results save folder\n",
        "if not os.path.isdir(added_path):\n",
        "    os.mkdir(added_path)\n",
        "if not os.path.isdir(added_path + 'Random_results'):\n",
        "    os.mkdir(added_path + 'Random_results')\n",
        "if not os.path.isdir(added_path + 'Fixed_results'):\n",
        "    os.mkdir(added_path + 'Fixed_results')\n",
        "\n",
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []\n",
        "\n",
        "do_compute_norm = False\n",
        "\n",
        "def train(model_G, model_D, epoch):\n",
        "\n",
        "    model_D.train()\n",
        "    model_G.train()\n",
        "    \n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "    \n",
        "    time_now = time.clock()\n",
        "\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "          \n",
        "        for d_index in range(d_steps):\n",
        "\n",
        "            # (1) Update Discriminator : maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            \n",
        "            # this is not necessarily equal to batch size !\n",
        "            n_images = len(inputs)\n",
        "            \n",
        "            # Reset gradients\n",
        "            model_D.zero_grad()\n",
        "\n",
        "            # on real samples\n",
        "            d_real_decision = model_D(inputs)\n",
        "\n",
        "            # To compute BCE loss\n",
        "            label = torch.full((n_images, 1, 1, 1), real_label, device=device)\n",
        "            # D(x) should be 1 for real samples\n",
        "            d_real_loss = loss_fn(d_real_decision, label)\n",
        "\n",
        "            # on fake samples\n",
        "            d_gen_input = g_noise_input_generator(n_images, dim_gen_input)\n",
        "            d_fake_data = model_G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "            d_fake_decision = model_D(d_fake_data)\n",
        "            # D(x) now wants to be as close as possible as fake label value\n",
        "            f_label = torch.full((n_images, 1, 1, 1), fake_label, device=device)\n",
        "            d_fake_loss = loss_fn(d_fake_decision, f_label)\n",
        "            \n",
        "            # Total loss\n",
        "            d_train_loss = d_fake_loss + d_real_loss\n",
        "            d_train_loss.backward()\n",
        "            d_optimizer.step()  # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "            \n",
        "            D_losses.append(d_train_loss.item())\n",
        "\n",
        "\n",
        "            if do_compute_norm:\n",
        "              # Compute the norm of gradient\n",
        "              total_norm = 0\n",
        "              for p in model_D.parameters():\n",
        "                  norm = p.grad.data.norm(2)\n",
        "                  total_norm += norm.item() ** 2\n",
        "              total_norm = np.sqrt(total_norm)\n",
        "              arr_gradient_D.append(total_norm)\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "            model_G.zero_grad()\n",
        "\n",
        "            gen_input = g_noise_input_generator(n_images, dim_gen_input)\n",
        "            gen_output = model_G(gen_input)\n",
        "            d_fake_decision = model_D(gen_output)\n",
        "\n",
        "            # The generator tries to get D(G(z)) near 1\n",
        "            label = torch.full((n_images, 1, 1, 1), real_label, device=device)\n",
        "            g_loss = loss_fn(d_fake_decision,  label)\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "            \n",
        "            G_losses.append(g_loss.item())\n",
        "\n",
        "            # Compute norm of gradient\n",
        "            if do_compute_norm:\n",
        "              total_norm = 0\n",
        "              for p in model_G.parameters():\n",
        "                  norm = p.grad.data.norm(2)\n",
        "                  total_norm += norm.item() ** 2\n",
        "              total_norm = np.sqrt(total_norm)\n",
        "              arr_gradient_G.append(total_norm)\n",
        "\n",
        "        if batch_idx % 10 == 0:            \n",
        "            # Store mean/std_dev            \n",
        "            if plot_means_and_var:\n",
        "                # Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\n",
        "                np_gen_output = gen_output.detach().numpy()\n",
        "                mean = np.mean(np_gen_output)\n",
        "                std_dev = np.std(np_gen_output)\n",
        "                arr_means.append(mean)\n",
        "                arr_std_dev.append(std_dev)\n",
        "                print('mean = ', mean, ' std_dev = ', std_dev)\n",
        "            \n",
        "            print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)] Time per epoch: {:.2f}s\\t Gen_Loss: {:.4f}; D_Real_Loss: {:.4f}; D_Fake_Loss: {:.4f}'\n",
        "                  .format(epoch, batch_idx * len(inputs), n_train_images,\n",
        "                          100. * batch_idx / len(train_loader), n_train_images / (10 * batch_size) * (time.clock() - time_now), \n",
        "                          d_real_loss.item(), d_fake_loss.item(), g_loss.item()),\n",
        "                  end='')        \n",
        "            time_now = time.clock()\n",
        "                 \n",
        "    return D_losses, G_losses\n",
        "\n",
        "  \n",
        "for ep in range(n_epochs):\n",
        "    D_losses, G_losses = train(model_G, model_D, ep)\n",
        "    \n",
        "    p = added_path + 'Random_results/MNIST_GAN_' + str(ep + 1) + '.png'\n",
        "    fixed_p = added_path + 'Fixed_results/MNIST_GAN_' + str(ep + 1) + '.png'\n",
        "    show_result(model_G, dim_gen_input, (ep + 1), save=True, path=p, isFix=False)\n",
        "    show_result(model_G, dim_gen_input, (ep + 1), save=True, path=fixed_p, isFix=True)\n",
        "    train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
        "    train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
        "\n",
        "    \n",
        "print(\"Training finish!... save training results\")\n",
        "torch.save(model_G.state_dict(), added_path + \"generator_param.pkl\")\n",
        "torch.save(model_D.state_dict(), added_path + \"discriminator_param.pkl\")\n",
        "with open(added_path + 'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path= added_path + 'MNIST_GAN_train_hist.png')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 19 [59000/60000 (98%)] Time per epoch: 712.85s\t Gen_Loss: 0.0364; D_Real_Loss: 0.0200; D_Fake_Loss: 8.5525Training finish!... save training results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E9CAyz0ssEdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate some animation..? (haven't used it)\n",
        "images = []\n",
        "for ep in range(n_epochs):\n",
        "    img_name = added_path + 'Fixed_results/MNIST_GAN_' + str(ep + 1) + '.png'\n",
        "    images.append(imageio.imread(img_name))\n",
        "imageio.mimsave(added_path + 'generation_animation.gif', images, fps=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4T3BvqyiPbDD",
        "colab_type": "code",
        "outputId": "cf212f50-9b10-4cfb-ca74-19228ccfe889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision.utils\n",
        "\n",
        "# need to install this to save images via pytorch.. \n",
        "!pip install Pillow==4.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D8RusX1TSRVk",
        "colab_type": "code",
        "outputId": "3746d708-009a-4091-93aa-01f01fd377ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Save 10k MNIST images to a folder if you haven't already done so\n",
        "# This will allow FID comparing\n",
        "\n",
        "def save_real_images_data_set(path):\n",
        "  \n",
        "  test_data = datasets.MNIST('../data', train=False, transform=transform)\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        "  )\n",
        "\n",
        "  time_now = time.clock() \n",
        "  n_test_data = len(test_data)\n",
        "\n",
        "  i = 0\n",
        "  for batch_idx, (inputs, target) in enumerate(test_loader):\n",
        "\n",
        "    n_inputs = len(inputs)\n",
        "\n",
        "    for j in range(n_inputs):\n",
        "        torchvision.utils.save_image(inputs[j, :].view(image_size, image_size), \n",
        "                                     path + 'img_' + str(i) + '.png', normalize = True)    \n",
        "        i += 1\n",
        "\n",
        "    print('\\r Real_Images_Saved [{}/{} ({:.0f}%)] Time estimated left: {:.2f}s'\n",
        "            .format(i, n_test_data, 100 * i / n_test_data, \n",
        "                    ((n_test_data - i) / batch_size) * (time.clock() - time_now)),   \n",
        "            end='')\n",
        "    time_now = time.clock()            \n",
        "   \n",
        "  \n",
        "path_real = added_path + 'real_samples/'    \n",
        "\n",
        "if not os.path.isdir(path_real):\n",
        "    os.mkdir(path_real)\n",
        "    save_real_images_data_set(path_real)\n",
        "else:\n",
        "    # don't compute twice !!\n",
        "    print('Real Samples folder is already there !')\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Real_Images_Saved [10000/10000 (100%)] Time estimated left: 0.00s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sMS8ObX7-6Iu",
        "colab_type": "code",
        "outputId": "cf87d5fb-80c1-4d09-9115-f76f52fcfa14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate 10k samples\n",
        "# by 100 batches of 100\n",
        "import torchvision.utils\n",
        "\n",
        "path_gen = added_path + 'gen_samples/'\n",
        "if not os.path.isdir(path_gen):\n",
        "    os.mkdir(path_gen)\n",
        "    \n",
        "\n",
        "time_now = time.clock() \n",
        "for i in range(100):\n",
        "  noise = g_noise_input_generator(100, dim_gen_input)\n",
        "  gen_images = model_G(noise)\n",
        "\n",
        "  for j in range(100):\n",
        "      torchvision.utils.save_image(gen_images[j, :].view(image_size, image_size), \n",
        "                        path_gen + 'img_' + str(i*100 + j) + '.png', normalize = True)    \n",
        "      \n",
        "  print('\\r Gen_Images_Saved [{}/{} ({:.0f}%)] Time estimated left: {:.2f}s'\n",
        "        .format((i + 1) * 100, 10000, (i + 1), (99 - i) * (time.clock() - time_now)),   \n",
        "         end='')\n",
        "  time_now = time.clock()\n",
        "          \n",
        "\n",
        "     \n",
        "  \n",
        "              \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Gen_Images_Saved [10000/10000 (1%)] Time estimated left: 0.00s"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}