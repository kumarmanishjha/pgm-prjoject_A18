{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1599,
     "status": "ok",
     "timestamp": 1544484796733,
     "user": {
      "displayName": "Manish Jha",
      "photoUrl": "",
      "userId": "03240707437467482388"
     },
     "user_tz": 300
    },
    "id": "Ic52o2bUVzmY",
    "outputId": "9d21e5d6-d8b5-4042-cca4-a5b93da01dd0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import GaussianNoise, Conv2D, Conv2DTranspose, Activation, UpSampling2D\n",
    "from keras.layers import Input, Flatten, BatchNormalization, Dense, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1544484799251,
     "user": {
      "displayName": "Manish Jha",
      "photoUrl": "",
      "userId": "03240707437467482388"
     },
     "user_tz": 300
    },
    "id": "YXdo-aldWJf0",
    "outputId": "0cec5a5b-3bba-4e00-eebe-5268354f39d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.4)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (1.11.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NfJuhukWSpL"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJ8ZqRaYWavA"
   },
   "outputs": [],
   "source": [
    "#https://drive.google.com/open?id=1onwJKf6gNbTaHDHeW3_n2GHSLOx_VL2q\n",
    "download = drive.CreateFile({'id': '1onwJKf6gNbTaHDHeW3_n2GHSLOx_VL2q'})\n",
    "download.GetContentFile('mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLO5p7DeVzmz"
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.input = Input([400])\n",
    "        layer = Reshape((2,2,100))(self.input)\n",
    "        #layer = Conv2DTranspose(1024,kernel_size= 4,strides = 4, padding = 'same')(layer)\n",
    "        #layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        #layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        #layer = Conv2DTranspose(512,kernel_size= 4,strides = 2, padding = 'same')(layer)\n",
    "        #layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        #layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Conv2DTranspose(256,kernel_size= 4,strides = 2, padding = 'same' )(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Conv2DTranspose(128,kernel_size= 4,strides = 2, padding = 'same')(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Conv2DTranspose(64,kernel_size= 4,strides = 2, padding = 'same' )(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = Activation(\"tanh\")(layer)\n",
    "        layer = Conv2DTranspose(3,kernel_size= 4,strides = 2, padding = 'same' )(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = Activation(\"tanh\")(layer)\n",
    "        #layer = Dense()\n",
    "        self.model = Model(inputs=self.input, outputs=layer)\n",
    "        #summary = self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 2, 2, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 4, 4, 256)         409856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 16, 16, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_31 (Conv2DT (None, 32, 32, 3)         3075      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,070,287\n",
      "Trainable params: 1,069,385\n",
      "Non-trainable params: 902\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKmbLyjEVzm_"
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        # discriminator \n",
    "        #self.input = Input(shape=(28,28,1))\n",
    "        #layer = Dense(784)(layer)\n",
    "        self.input = Input([32**2 * 3])        \n",
    "        layer = Reshape((32,32,3))(self.input)\n",
    "        layer = Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        #layer = Dropout(0.25)(layer)\n",
    "        layer = Conv2D(256, kernel_size=4, strides=2, padding=\"same\")(layer)\n",
    "        layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        #layer = Dropout(0.25)(layer)\n",
    "        #layer = Conv2D(512, kernel_size=4, strides=2, padding=\"same\")(layer)\n",
    "        #layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        #layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        #layer = Dropout(0.25)(layer)\n",
    "        #layer = Conv2D(1024, kernel_size=4, strides=2, padding=\"same\", activation = 'sigmoid')(layer)\n",
    "        #layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        #layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Flatten()(layer)\n",
    "        layer = Dense(250)(layer)\n",
    "        self.feature = Model(inputs=self.input, outputs=layer)\n",
    "        self.output = Dense(10, activation='sigmoid')(layer)\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "        #summary = self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        3136      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               1024250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2510      \n",
      "=================================================================\n",
      "Total params: 1,687,432\n",
      "Trainable params: 1,686,536\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CC4Maz-bVznO"
   },
   "outputs": [],
   "source": [
    "def noise_gen(batch_size, z_dim):\n",
    "    noise = np.zeros((batch_size, z_dim), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        noise[i, :] = np.random.uniform(0, 1, z_dim)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxyzfDF7VznW"
   },
   "outputs": [],
   "source": [
    "def plot_images(Generator, save2file=False, samples=25, step=0):\n",
    "    filename = \"./images/mnist_%d.png\" % step\n",
    "    #noise = np.random.normal(0, 1, (samples, 100))\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[samples, 400])\n",
    "    images = Generator.predict(noise)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        #image = images[i, :, :, :]\n",
    "        image = images[i, :]\n",
    "        image = np.reshape(image, [28, 28])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save2file:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8H-E_GbVznf"
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "seed_data = 3\n",
    "unlabeled_weight = 1\n",
    "batch_size = 100\n",
    "count =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5jO6DBGVznm"
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "data = np.load('mnist.npz')\n",
    "trainx = np.concatenate([data['x_train'], data['x_valid']], axis=0)\n",
    "trainx_unl = trainx.copy()\n",
    "trainx_unl2 = trainx.copy()\n",
    "trainy = np.concatenate([data['y_train'], data['y_valid']]).astype(np.int32)\n",
    "#nr_batches_train = int(trainx.shape[0] / args.batch_size)\n",
    "nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "testx = data['x_test']\n",
    "testy = data['y_test'].astype(np.int32)\n",
    "testy = np.reshape(testy, [testy.shape[0], 1])\n",
    "#nr_batches_test = int(testx.shape[0] / args.batch_size)\n",
    "nr_batches_test = int(testx.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,), (10000, 784), (10000, 1), 500, 100)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load cifar 10\n",
    "(trainx, trainy), (testx, testy) = cifar10.load_data()\n",
    "trainx = trainx.reshape(50000,32*32*3)\n",
    "trainy = np.ravel(trainy)\n",
    "testx = trainx.reshape(50000,32*32*3)\n",
    "testy = np.ravel(testy)\n",
    "trainx_unl = trainx.copy()\n",
    "trainx_unl2 = trainx.copy()\n",
    "#nr_batches_train = int(trainx.shape[0] / args.batch_size)\n",
    "nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "testx = data['x_test']\n",
    "testy = data['y_test'].astype(np.int32)\n",
    "testy = np.reshape(testy, [testy.shape[0], 1])\n",
    "#nr_batches_test = int(testx.shape[0] / args.batch_size)\n",
    "nr_batches_test = int(testx.shape[0] / batch_size)\n",
    "trainx.shape, trainy.shape, testx.shape, testy.shape, nr_batches_train, nr_batches_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 4, 1, 1, 2, 7, 8, 3], dtype=uint8)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CC7rhAbVznz"
   },
   "outputs": [],
   "source": [
    "# select labeled data\n",
    "rng = np.random.RandomState(seed)\n",
    "data_rng = np.random.RandomState(seed_data)\n",
    "inds = data_rng.permutation(trainx.shape[0])\n",
    "trainx = trainx[inds]\n",
    "trainy = trainy[inds]\n",
    "txs = []\n",
    "tys = []\n",
    "for j in range(10):\n",
    "    txs.append(trainx[trainy == j][:count])\n",
    "    tys.append(trainy[trainy == j][:count])\n",
    "txs = np.concatenate(txs, axis=0)\n",
    "tys = np.concatenate(tys, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJg6lkyaVzn8"
   },
   "outputs": [],
   "source": [
    "# set up tensorflow and keras\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkYLAbyqVzoV"
   },
   "outputs": [],
   "source": [
    "# network\n",
    "discriminator_model = Discriminator()\n",
    "discriminator = discriminator_model.model\n",
    "discriminator_feature = discriminator_model.feature\n",
    "generator_model = Generator()\n",
    "generator = generator_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0jjbme8Vzom"
   },
   "outputs": [],
   "source": [
    "#### loss function computation\n",
    "x_label = Input([32**2*3])\n",
    "x_unlabel = Input([32**2*3])\n",
    "labels = Input([1], dtype=tf.int32)\n",
    "noise = Input([400])\n",
    "fake_image = generator(noise)\n",
    "output_before_softmax_label = discriminator(x_label)\n",
    "output_before_softmax_unlabel = discriminator(x_unlabel)\n",
    "output_before_softmax_fake = discriminator(fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oL5ZXQpCVzow"
   },
   "outputs": [],
   "source": [
    "z_exp_label = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_label))\n",
    "z_exp_unlabel = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_unlabel))\n",
    "z_exp_fake = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_fake))\n",
    "index_flattened = tf.range(0, batch_size) * output_before_softmax_label.shape[1] + tf.reshape(labels, [batch_size])\n",
    "l_label = tf.gather(tf.reshape(output_before_softmax_label, [-1]), index_flattened)\n",
    "l_unlabel = tf.reduce_logsumexp(output_before_softmax_unlabel)\n",
    "loss_label = -tf.reduce_mean(l_label) + tf.reduce_mean(z_exp_label)\n",
    "loss_unlabel = -0.5 * tf.reduce_mean(l_unlabel) + 0.5 * tf.reduce_mean(tf.nn.softplus(l_unlabel)) + \\\n",
    "               0.5 * tf.reduce_mean(tf.nn.softplus(tf.reduce_logsumexp(output_before_softmax_fake)))\n",
    "loss_discriminator = tf.add(loss_label, tf.multiply(loss_unlabel, unlabeled_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J834r8CRVzo4"
   },
   "outputs": [],
   "source": [
    "feature_generated = tf.reduce_mean(discriminator_feature(fake_image), axis=0)\n",
    "feature_real = tf.reduce_mean(discriminator_feature(x_unlabel), axis=0)\n",
    "loss_generator = tf.reduce_mean(tf.square(feature_generated - feature_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6O6PpEGVzpD"
   },
   "outputs": [],
   "source": [
    "train_err = tf.reduce_mean(\n",
    "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))\n",
    "test_error = tf.reduce_mean(\n",
    "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7794,
     "status": "ok",
     "timestamp": 1544486103530,
     "user": {
      "displayName": "Manish Jha",
      "photoUrl": "",
      "userId": "03240707437467482388"
     },
     "user_tz": 300
    },
    "id": "Nsjf0oa0VzpK",
    "outputId": "4a35fa4b-d870-4b78-9bdc-c41b13a75fc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# train settings\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "discriminator_gradients = discriminator_optimizer.compute_gradients(loss_discriminator, discriminator.trainable_weights)\n",
    "# discriminator_grads_and_vars = zip(discriminator_gradients, discriminator.trainable_weights)\n",
    "discriminator_train = discriminator_optimizer.apply_gradients(discriminator_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "246futDyVzpY"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "generator_gradients = generator_optimizer.compute_gradients(loss_generator, generator.trainable_weights)\n",
    "# generator_grads_and_vars = zip(generator_gradients, generator.trainable_weights)\n",
    "generator_train = generator_optimizer.apply_gradients(generator_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQcJAFL6Vzpi"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1505895,
     "status": "error",
     "timestamp": 1544487601707,
     "user": {
      "displayName": "Manish Jha",
      "photoUrl": "",
      "userId": "03240707437467482388"
     },
     "user_tz": 300
    },
    "id": "V4DzeMd3Vzpx",
    "outputId": "41702b84-4b71-4426-8dc9-74cd3bb2b4c3"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-7821288738be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         _, loss_generator_this = sess.run([generator_train, loss_generator], feed_dict={\n\u001b[1;32m     37\u001b[0m             \u001b[0mnoise\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnoise_feed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx_unlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainx_unl2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         })\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss_label_record\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnr_batches_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    begin = time.time()\n",
    "\n",
    "    # construct randomly permuted minibatches\n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    for t in range(int(trainx_unl.shape[0] / txs.shape[0])):\n",
    "        inds = rng.permutation(txs.shape[0])\n",
    "        trainx.append(txs[inds])\n",
    "        trainy.append(tys[inds])\n",
    "    trainx = np.concatenate(trainx, axis=0)\n",
    "    trainy = np.concatenate(trainy, axis=0)\n",
    "    trainy = np.reshape(trainy, [trainy.shape[0], 1])\n",
    "    trainx_unl = trainx_unl[rng.permutation(trainx_unl.shape[0])]\n",
    "    trainx_unl2 = trainx_unl2[rng.permutation(trainx_unl2.shape[0])]\n",
    "\n",
    "    # train\n",
    "    loss_label_record = 0.\n",
    "    loss_unlabel_record = 0.\n",
    "    train_err_record = 0.\n",
    "    for t in range(nr_batches_train):\n",
    "        noise_feed = noise_gen(batch_size, 400)\n",
    "        _, loss_label_this, loss_unlabel_this, train_err_this = sess.run(\n",
    "            [discriminator_train, loss_label, loss_unlabel, train_err], feed_dict={\n",
    "                x_label: trainx[t * batch_size:(t + 1) * batch_size],\n",
    "                x_unlabel: trainx_unl[t * batch_size:(t + 1) * batch_size],\n",
    "                labels: trainy[t * batch_size:(t + 1) * batch_size],\n",
    "                noise: noise_feed\n",
    "            })\n",
    "\n",
    "        loss_label_record += loss_label_this\n",
    "        loss_unlabel_record += loss_unlabel_this\n",
    "        train_err_record += train_err_this\n",
    "\n",
    "        noise_feed = noise_gen(batch_size, 400)\n",
    "        _, loss_generator_this = sess.run([generator_train, loss_generator], feed_dict={\n",
    "            noise: noise_feed,\n",
    "            x_unlabel: trainx_unl2[t * batch_size:(t + 1) * batch_size]\n",
    "        })\n",
    "    loss_label_record /= nr_batches_train\n",
    "    loss_unlabel_record /= nr_batches_train\n",
    "    train_err_record /= nr_batches_train\n",
    "\n",
    "    # test\n",
    "    test_err_record = 0.\n",
    "    for t in range(nr_batches_test):\n",
    "        test_err_this = sess.run(test_error, feed_dict={\n",
    "            x_label: testx[t * batch_size:(t + 1) * batch_size],\n",
    "            labels: testy[t * batch_size:(t + 1) * batch_size]\n",
    "        })\n",
    "        test_err_record += test_err_this\n",
    "    test_err_record /= nr_batches_test\n",
    "\n",
    "    # report\n",
    "    if (epoch%5 == 0):\n",
    "      print(\"Iteration %d, time = %ds, loss_lab = %.4f, loss_unl = %.4f, train err = %.4f, test err = %.4f\" % (\n",
    "          epoch, time.time() - begin, loss_label_record, loss_unlabel_record, train_err_record, test_err_record))\n",
    "      plot_images(generator)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTrEozBzVzqI"
   },
   "outputs": [],
   "source": [
    "plot_images(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIT28YZzaeq5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of fm_gans_v3.ipynb",
   "provenance": [
    {
     "file_id": "1Rp58AG0HFjWzUPPdYCqM_Dsr8oUxbPe9",
     "timestamp": 1544489398616
    },
    {
     "file_id": "1te-p5mULLrhkTa3_WdyfrI5p3Mb3ksLD",
     "timestamp": 1544488356506
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
