{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fm_gan_new.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2qYqXIUvjwBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7679f1-e1c6-4a56-fbb8-52227d2144e4"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.layers import Input, BatchNormalization, Dense, GaussianNoise\n",
        "from keras.models import Model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_aYagHykZtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "81ded5ee-dafa-4be2-ad9c-86ec09e99a7b"
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Running setup.py bdist_wheel for pydrive ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "huK7BpOgkUvx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PGmu-rRikm7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1XCsuyjLP2l04S9uNGIyOAbfeADU_mcJv'})\n",
        "download.GetContentFile('mnist.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8CtnGONjwBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "    def __init__(self):\n",
        "        # generator\n",
        "        self.input = Input([100])\n",
        "        layer = Dense(500, activation=K.softplus)(self.input)\n",
        "        layer = BatchNormalization()(layer)\n",
        "        layer = Dense(500, activation=K.softplus)(layer)\n",
        "        layer = BatchNormalization()(layer)\n",
        "        self.output = Dense(28 ** 2, activation=K.sigmoid, kernel_regularizer=regularizers.l2())(layer)\n",
        "        self.model = Model(inputs=self.input, outputs=self.output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVGw6peVjwBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator:\n",
        "    def __init__(self):\n",
        "        # discriminator\n",
        "        self.input = Input([28 ** 2])\n",
        "        layer = GaussianNoise(stddev=0.3)(self.input)\n",
        "        layer = Dense(1000)(layer)\n",
        "        layer = GaussianNoise(stddev=0.5)(layer)\n",
        "        layer = Dense(500)(layer)\n",
        "        layer = GaussianNoise(stddev=0.5)(layer)\n",
        "        layer = Dense(250)(layer)\n",
        "        layer = GaussianNoise(stddev=0.5)(layer)\n",
        "        layer = Dense(250)(layer)\n",
        "        layer = GaussianNoise(stddev=0.5)(layer)\n",
        "        layer = Dense(250)(layer)\n",
        "        self.feature = Model(inputs=self.input, outputs=layer)\n",
        "        layer = GaussianNoise(stddev=0.5)(layer)\n",
        "        self.output = Dense(10)(layer)\n",
        "        self.model = Model(inputs=self.input, outputs=self.output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJ1LrwgajwBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def noise_gen(batch_size, z_dim):\n",
        "    noise = np.zeros((batch_size, z_dim), dtype=np.float32)\n",
        "    for i in range(batch_size):\n",
        "        noise[i, :] = np.random.uniform(0, 1, z_dim)\n",
        "    return noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7TfoGrojwBf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# arguments\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--seed', type=int, default=1)\n",
        "#parser.add_argument('--seed_data', type=int, default=1)\n",
        "#parser.add_argument('--unlabeled_weight', type=float, default=1.)\n",
        "#parser.add_argument('--batch_size', type=int, default=100)\n",
        "#parser.add_argument('--count', type=int, default=10)\n",
        "#args = parser.parse_args()\n",
        "#print(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNg0ABF2jwBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "seed_data = 1\n",
        "unlabeled_weight = 1\n",
        "batch_size = 100\n",
        "count =10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vu31NlgjwBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load MNIST data\n",
        "data = np.load('mnist.npz')\n",
        "trainx = np.concatenate([data['x_train'], data['x_valid']], axis=0)\n",
        "trainx_unl = trainx.copy()\n",
        "trainx_unl2 = trainx.copy()\n",
        "trainy = np.concatenate([data['y_train'], data['y_valid']]).astype(np.int32)\n",
        "#nr_batches_train = int(trainx.shape[0] / args.batch_size)\n",
        "nr_batches_train = int(trainx.shape[0] / batch_size)\n",
        "testx = data['x_test']\n",
        "testy = data['y_test'].astype(np.int32)\n",
        "testy = np.reshape(testy, [testy.shape[0], 1])\n",
        "#nr_batches_test = int(testx.shape[0] / args.batch_size)\n",
        "nr_batches_test = int(testx.shape[0] / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vt6QxxYKjwBq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# select labeled data\n",
        "rng = np.random.RandomState(seed)\n",
        "data_rng = np.random.RandomState(seed_data)\n",
        "inds = data_rng.permutation(trainx.shape[0])\n",
        "trainx = trainx[inds]\n",
        "trainy = trainy[inds]\n",
        "txs = []\n",
        "tys = []\n",
        "for j in range(10):\n",
        "    txs.append(trainx[trainy == j][:count])\n",
        "    tys.append(trainy[trainy == j][:count])\n",
        "txs = np.concatenate(txs, axis=0)\n",
        "tys = np.concatenate(tys, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WmM2AjvjwBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set up tensorflow and keras\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IshT5gaZjwB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# network\n",
        "discriminator_model = Discriminator()\n",
        "discriminator = discriminator_model.model\n",
        "discriminator_feature = discriminator_model.feature\n",
        "generator_model = Generator()\n",
        "generator = generator_model.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwNOL-pmjwB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss function computation\n",
        "x_label = Input([28 ** 2])\n",
        "x_unlabel = Input([28 ** 2])\n",
        "labels = Input([1], dtype=tf.int32)\n",
        "noise = Input([100])\n",
        "fake_image = generator(noise)\n",
        "output_before_softmax_label = discriminator(x_label)\n",
        "output_before_softmax_unlabel = discriminator(x_unlabel)\n",
        "output_before_softmax_fake = discriminator(fake_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOA7acs0jwB6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z_exp_label = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_label))\n",
        "z_exp_unlabel = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_unlabel))\n",
        "z_exp_fake = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_fake))\n",
        "index_flattened = tf.range(0, batch_size) * output_before_softmax_label.shape[1] + tf.reshape(labels, [batch_size])\n",
        "l_label = tf.gather(tf.reshape(output_before_softmax_label, [-1]), index_flattened)\n",
        "l_unlabel = tf.reduce_logsumexp(output_before_softmax_unlabel)\n",
        "loss_label = -tf.reduce_mean(l_label) + tf.reduce_mean(z_exp_label)\n",
        "loss_unlabel = -0.5 * tf.reduce_mean(l_unlabel) + 0.5 * tf.reduce_mean(tf.nn.softplus(l_unlabel)) + \\\n",
        "               0.5 * tf.reduce_mean(tf.nn.softplus(tf.reduce_logsumexp(output_before_softmax_fake)))\n",
        "loss_discriminator = tf.add(loss_label, tf.multiply(loss_unlabel, unlabeled_weight))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RhVObaIjwB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feature_generated = tf.reduce_mean(discriminator_feature(fake_image), axis=0)\n",
        "feature_real = tf.reduce_mean(discriminator_feature(x_unlabel), axis=0)\n",
        "loss_generator = tf.reduce_mean(tf.square(feature_generated - feature_real))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4D1Qzm2LjwCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_err = tf.reduce_mean(\n",
        "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))\n",
        "test_error = tf.reduce_mean(\n",
        "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLK--UTxjwCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "dabab8df-1f14-4d46-fe41-b797fe289712"
      },
      "cell_type": "code",
      "source": [
        "# train settings\n",
        "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "discriminator_gradients = discriminator_optimizer.compute_gradients(loss_discriminator, discriminator.trainable_weights)\n",
        "# discriminator_grads_and_vars = zip(discriminator_gradients, discriminator.trainable_weights)\n",
        "discriminator_train = discriminator_optimizer.apply_gradients(discriminator_gradients)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "burZpRbjjwCL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "generator_gradients = generator_optimizer.compute_gradients(loss_generator, generator.trainable_weights)\n",
        "# generator_grads_and_vars = zip(generator_gradients, generator.trainable_weights)\n",
        "generator_train = generator_optimizer.apply_gradients(generator_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGW_9z2ZjwCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8U6heFXjwCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5417
        },
        "outputId": "9d2dd18b-4e0f-497f-983e-f2978083a10e"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(300):\n",
        "    begin = time.time()\n",
        "\n",
        "    # construct randomly permuted minibatches\n",
        "    trainx = []\n",
        "    trainy = []\n",
        "    for t in range(int(trainx_unl.shape[0] / txs.shape[0])):\n",
        "        inds = rng.permutation(txs.shape[0])\n",
        "        trainx.append(txs[inds])\n",
        "        trainy.append(tys[inds])\n",
        "    trainx = np.concatenate(trainx, axis=0)\n",
        "    trainy = np.concatenate(trainy, axis=0)\n",
        "    trainy = np.reshape(trainy, [trainy.shape[0], 1])\n",
        "    trainx_unl = trainx_unl[rng.permutation(trainx_unl.shape[0])]\n",
        "    trainx_unl2 = trainx_unl2[rng.permutation(trainx_unl2.shape[0])]\n",
        "\n",
        "    # train\n",
        "    loss_label_record = 0.\n",
        "    loss_unlabel_record = 0.\n",
        "    train_err_record = 0.\n",
        "    for t in range(nr_batches_train):\n",
        "        noise_feed = noise_gen(batch_size, 100)\n",
        "        _, loss_label_this, loss_unlabel_this, train_err_this = sess.run(\n",
        "            [discriminator_train, loss_label, loss_unlabel, train_err], feed_dict={\n",
        "                x_label: trainx[t * batch_size:(t + 1) * batch_size],\n",
        "                x_unlabel: trainx_unl[t * batch_size:(t + 1) * batch_size],\n",
        "                labels: trainy[t * batch_size:(t + 1) * batch_size],\n",
        "                noise: noise_feed\n",
        "            })\n",
        "\n",
        "        loss_label_record += loss_label_this\n",
        "        loss_unlabel_record += loss_unlabel_this\n",
        "        train_err_record += train_err_this\n",
        "\n",
        "        noise_feed = noise_gen(batch_size, 100)\n",
        "        _, loss_generator_this = sess.run([generator_train, loss_generator], feed_dict={\n",
        "            noise: noise_feed,\n",
        "            x_unlabel: trainx_unl2[t * batch_size:(t + 1) * batch_size]\n",
        "        })\n",
        "    loss_label_record /= nr_batches_train\n",
        "    loss_unlabel_record /= nr_batches_train\n",
        "    train_err_record /= nr_batches_train\n",
        "\n",
        "    # test\n",
        "    test_err_record = 0.\n",
        "    for t in range(nr_batches_test):\n",
        "        test_err_this = sess.run(test_error, feed_dict={\n",
        "            x_label: testx[t * batch_size:(t + 1) * batch_size],\n",
        "            labels: testy[t * batch_size:(t + 1) * batch_size]\n",
        "        })\n",
        "        test_err_record += test_err_this\n",
        "    test_err_record /= nr_batches_test\n",
        "\n",
        "    # report\n",
        "    if (epoch%100 = 0):\n",
        "      print(\"Iteration %d, time = %ds, loss_lab = %.4f, loss_unl = %.4f, train err = %.4f, test err = %.4f\" % (\n",
        "          epoch, time.time() - begin, loss_label_record, loss_unlabel_record, train_err_record, test_err_record))\n",
        "      plot_images(generator)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, time = 10s, loss_lab = 6.8137, loss_unl = 0.5532, train err = 0.2790, test err = 0.3787\n",
            "Iteration 1, time = 9s, loss_lab = 5.8442, loss_unl = 0.2848, train err = 0.0948, test err = 0.3382\n",
            "Iteration 2, time = 9s, loss_lab = 5.5411, loss_unl = 0.2057, train err = 0.0479, test err = 0.3403\n",
            "Iteration 3, time = 9s, loss_lab = 5.3620, loss_unl = 0.1410, train err = 0.0284, test err = 0.3327\n",
            "Iteration 4, time = 9s, loss_lab = 5.2990, loss_unl = 0.1271, train err = 0.0228, test err = 0.2915\n",
            "Iteration 5, time = 9s, loss_lab = 5.2255, loss_unl = 0.1166, train err = 0.0169, test err = 0.3079\n",
            "Iteration 6, time = 9s, loss_lab = 5.1833, loss_unl = 0.1077, train err = 0.0153, test err = 0.2937\n",
            "Iteration 7, time = 9s, loss_lab = 5.1610, loss_unl = 0.1011, train err = 0.0134, test err = 0.2936\n",
            "Iteration 8, time = 9s, loss_lab = 5.1392, loss_unl = 0.0926, train err = 0.0126, test err = 0.3068\n",
            "Iteration 9, time = 9s, loss_lab = 5.1243, loss_unl = 0.0895, train err = 0.0121, test err = 0.2935\n",
            "Iteration 10, time = 9s, loss_lab = 5.1135, loss_unl = 0.0871, train err = 0.0114, test err = 0.3076\n",
            "Iteration 11, time = 9s, loss_lab = 5.0918, loss_unl = 0.0801, train err = 0.0103, test err = 0.3026\n",
            "Iteration 12, time = 9s, loss_lab = 5.0918, loss_unl = 0.0834, train err = 0.0116, test err = 0.2975\n",
            "Iteration 13, time = 9s, loss_lab = 5.0788, loss_unl = 0.0784, train err = 0.0101, test err = 0.3016\n",
            "Iteration 14, time = 9s, loss_lab = 5.0691, loss_unl = 0.0790, train err = 0.0101, test err = 0.2968\n",
            "Iteration 15, time = 9s, loss_lab = 5.0643, loss_unl = 0.0779, train err = 0.0097, test err = 0.3041\n",
            "Iteration 16, time = 9s, loss_lab = 5.0548, loss_unl = 0.0730, train err = 0.0092, test err = 0.2979\n",
            "Iteration 17, time = 9s, loss_lab = 5.0618, loss_unl = 0.0811, train err = 0.0100, test err = 0.3226\n",
            "Iteration 18, time = 9s, loss_lab = 5.0519, loss_unl = 0.0826, train err = 0.0095, test err = 0.2949\n",
            "Iteration 19, time = 9s, loss_lab = 5.0435, loss_unl = 0.0732, train err = 0.0088, test err = 0.3097\n",
            "Iteration 20, time = 9s, loss_lab = 5.0204, loss_unl = 0.0718, train err = 0.0074, test err = 0.3107\n",
            "Iteration 21, time = 9s, loss_lab = 5.0225, loss_unl = 0.0714, train err = 0.0073, test err = 0.3011\n",
            "Iteration 22, time = 9s, loss_lab = 5.0203, loss_unl = 0.0681, train err = 0.0075, test err = 0.2986\n",
            "Iteration 23, time = 9s, loss_lab = 5.0179, loss_unl = 0.0663, train err = 0.0081, test err = 0.3018\n",
            "Iteration 24, time = 9s, loss_lab = 5.0105, loss_unl = 0.0698, train err = 0.0074, test err = 0.3092\n",
            "Iteration 25, time = 9s, loss_lab = 5.0144, loss_unl = 0.0676, train err = 0.0078, test err = 0.3042\n",
            "Iteration 26, time = 9s, loss_lab = 5.0072, loss_unl = 0.0653, train err = 0.0073, test err = 0.2943\n",
            "Iteration 27, time = 9s, loss_lab = 5.0111, loss_unl = 0.0664, train err = 0.0065, test err = 0.3001\n",
            "Iteration 28, time = 9s, loss_lab = 5.0146, loss_unl = 0.0666, train err = 0.0076, test err = 0.3001\n",
            "Iteration 29, time = 9s, loss_lab = 4.9945, loss_unl = 0.0590, train err = 0.0072, test err = 0.3029\n",
            "Iteration 30, time = 9s, loss_lab = 4.9994, loss_unl = 0.0626, train err = 0.0072, test err = 0.2982\n",
            "Iteration 31, time = 9s, loss_lab = 5.0034, loss_unl = 0.0679, train err = 0.0070, test err = 0.2990\n",
            "Iteration 32, time = 9s, loss_lab = 4.9946, loss_unl = 0.0632, train err = 0.0071, test err = 0.3147\n",
            "Iteration 33, time = 9s, loss_lab = 4.9926, loss_unl = 0.0612, train err = 0.0067, test err = 0.3026\n",
            "Iteration 34, time = 9s, loss_lab = 4.9894, loss_unl = 0.0625, train err = 0.0064, test err = 0.2924\n",
            "Iteration 35, time = 9s, loss_lab = 4.9892, loss_unl = 0.0598, train err = 0.0062, test err = 0.3078\n",
            "Iteration 36, time = 9s, loss_lab = 4.9892, loss_unl = 0.0590, train err = 0.0068, test err = 0.3052\n",
            "Iteration 37, time = 9s, loss_lab = 4.9890, loss_unl = 0.0659, train err = 0.0066, test err = 0.2971\n",
            "Iteration 38, time = 9s, loss_lab = 4.9805, loss_unl = 0.0582, train err = 0.0060, test err = 0.3035\n",
            "Iteration 39, time = 9s, loss_lab = 4.9883, loss_unl = 0.0619, train err = 0.0060, test err = 0.2993\n",
            "Iteration 40, time = 9s, loss_lab = 4.9820, loss_unl = 0.0666, train err = 0.0067, test err = 0.2960\n",
            "Iteration 41, time = 9s, loss_lab = 4.9883, loss_unl = 0.0628, train err = 0.0066, test err = 0.2894\n",
            "Iteration 42, time = 9s, loss_lab = 4.9821, loss_unl = 0.0602, train err = 0.0066, test err = 0.2912\n",
            "Iteration 43, time = 9s, loss_lab = 4.9814, loss_unl = 0.0620, train err = 0.0062, test err = 0.3081\n",
            "Iteration 44, time = 9s, loss_lab = 4.9784, loss_unl = 0.0617, train err = 0.0061, test err = 0.2970\n",
            "Iteration 45, time = 9s, loss_lab = 4.9772, loss_unl = 0.0568, train err = 0.0066, test err = 0.2936\n",
            "Iteration 46, time = 9s, loss_lab = 4.9872, loss_unl = 0.0588, train err = 0.0070, test err = 0.3052\n",
            "Iteration 47, time = 9s, loss_lab = 4.9692, loss_unl = 0.0560, train err = 0.0058, test err = 0.2970\n",
            "Iteration 48, time = 9s, loss_lab = 4.9725, loss_unl = 0.0633, train err = 0.0055, test err = 0.3079\n",
            "Iteration 49, time = 9s, loss_lab = 4.9737, loss_unl = 0.0563, train err = 0.0063, test err = 0.2921\n",
            "Iteration 50, time = 9s, loss_lab = 4.9755, loss_unl = 0.0583, train err = 0.0060, test err = 0.2979\n",
            "Iteration 51, time = 9s, loss_lab = 4.9799, loss_unl = 0.0616, train err = 0.0063, test err = 0.2903\n",
            "Iteration 52, time = 9s, loss_lab = 4.9770, loss_unl = 0.0595, train err = 0.0066, test err = 0.2904\n",
            "Iteration 53, time = 9s, loss_lab = 4.9723, loss_unl = 0.0607, train err = 0.0059, test err = 0.2941\n",
            "Iteration 54, time = 9s, loss_lab = 4.9695, loss_unl = 0.0566, train err = 0.0063, test err = 0.3023\n",
            "Iteration 55, time = 9s, loss_lab = 4.9729, loss_unl = 0.0588, train err = 0.0066, test err = 0.2994\n",
            "Iteration 56, time = 9s, loss_lab = 4.9629, loss_unl = 0.0592, train err = 0.0061, test err = 0.2979\n",
            "Iteration 57, time = 9s, loss_lab = 4.9663, loss_unl = 0.0576, train err = 0.0061, test err = 0.3020\n",
            "Iteration 58, time = 9s, loss_lab = 4.9706, loss_unl = 0.0612, train err = 0.0062, test err = 0.2980\n",
            "Iteration 59, time = 9s, loss_lab = 4.9669, loss_unl = 0.0575, train err = 0.0058, test err = 0.3068\n",
            "Iteration 60, time = 9s, loss_lab = 4.9723, loss_unl = 0.0608, train err = 0.0061, test err = 0.3046\n",
            "Iteration 61, time = 9s, loss_lab = 4.9637, loss_unl = 0.0547, train err = 0.0060, test err = 0.2949\n",
            "Iteration 62, time = 9s, loss_lab = 4.9646, loss_unl = 0.0568, train err = 0.0059, test err = 0.2865\n",
            "Iteration 63, time = 9s, loss_lab = 4.9649, loss_unl = 0.0588, train err = 0.0056, test err = 0.3002\n",
            "Iteration 64, time = 9s, loss_lab = 4.9712, loss_unl = 0.0591, train err = 0.0066, test err = 0.2864\n",
            "Iteration 65, time = 9s, loss_lab = 4.9700, loss_unl = 0.0572, train err = 0.0060, test err = 0.3032\n",
            "Iteration 66, time = 9s, loss_lab = 4.9630, loss_unl = 0.0570, train err = 0.0058, test err = 0.2929\n",
            "Iteration 67, time = 9s, loss_lab = 4.9657, loss_unl = 0.0562, train err = 0.0062, test err = 0.2975\n",
            "Iteration 68, time = 9s, loss_lab = 4.9607, loss_unl = 0.0574, train err = 0.0059, test err = 0.2978\n",
            "Iteration 69, time = 9s, loss_lab = 4.9610, loss_unl = 0.0565, train err = 0.0059, test err = 0.2975\n",
            "Iteration 70, time = 9s, loss_lab = 4.9645, loss_unl = 0.0532, train err = 0.0059, test err = 0.3071\n",
            "Iteration 71, time = 9s, loss_lab = 4.9561, loss_unl = 0.0543, train err = 0.0057, test err = 0.2979\n",
            "Iteration 72, time = 9s, loss_lab = 4.9568, loss_unl = 0.0521, train err = 0.0059, test err = 0.2904\n",
            "Iteration 73, time = 9s, loss_lab = 4.9637, loss_unl = 0.0536, train err = 0.0063, test err = 0.3071\n",
            "Iteration 74, time = 9s, loss_lab = 4.9674, loss_unl = 0.0586, train err = 0.0060, test err = 0.3031\n",
            "Iteration 75, time = 9s, loss_lab = 4.9626, loss_unl = 0.0563, train err = 0.0060, test err = 0.2865\n",
            "Iteration 76, time = 9s, loss_lab = 4.9623, loss_unl = 0.0581, train err = 0.0062, test err = 0.3039\n",
            "Iteration 77, time = 9s, loss_lab = 4.9541, loss_unl = 0.0569, train err = 0.0058, test err = 0.2970\n",
            "Iteration 78, time = 9s, loss_lab = 4.9631, loss_unl = 0.0609, train err = 0.0061, test err = 0.3057\n",
            "Iteration 79, time = 9s, loss_lab = 4.9540, loss_unl = 0.0547, train err = 0.0054, test err = 0.2911\n",
            "Iteration 80, time = 9s, loss_lab = 4.9521, loss_unl = 0.0553, train err = 0.0059, test err = 0.2942\n",
            "Iteration 81, time = 9s, loss_lab = 4.9584, loss_unl = 0.0591, train err = 0.0063, test err = 0.3007\n",
            "Iteration 82, time = 9s, loss_lab = 4.9588, loss_unl = 0.0561, train err = 0.0058, test err = 0.2961\n",
            "Iteration 83, time = 9s, loss_lab = 11.6503, loss_unl = 1.6962, train err = 0.1239, test err = 0.3025\n",
            "Iteration 84, time = 9s, loss_lab = 4.9840, loss_unl = 0.0634, train err = 0.0071, test err = 0.3012\n",
            "Iteration 85, time = 9s, loss_lab = 4.9669, loss_unl = 0.0594, train err = 0.0056, test err = 0.2915\n",
            "Iteration 86, time = 9s, loss_lab = 4.9562, loss_unl = 0.0566, train err = 0.0063, test err = 0.2947\n",
            "Iteration 87, time = 9s, loss_lab = 4.9590, loss_unl = 0.0539, train err = 0.0060, test err = 0.3078\n",
            "Iteration 88, time = 9s, loss_lab = 4.9547, loss_unl = 0.0581, train err = 0.0060, test err = 0.3044\n",
            "Iteration 89, time = 9s, loss_lab = 4.9553, loss_unl = 0.0556, train err = 0.0074, test err = 0.3012\n",
            "Iteration 90, time = 9s, loss_lab = 4.9534, loss_unl = 0.0567, train err = 0.0063, test err = 0.3023\n",
            "Iteration 91, time = 9s, loss_lab = 4.9582, loss_unl = 0.0586, train err = 0.0059, test err = 0.3005\n",
            "Iteration 92, time = 9s, loss_lab = 4.9523, loss_unl = 0.0538, train err = 0.0061, test err = 0.3000\n",
            "Iteration 93, time = 9s, loss_lab = 4.9580, loss_unl = 0.0586, train err = 0.0059, test err = 0.3002\n",
            "Iteration 94, time = 9s, loss_lab = 4.9544, loss_unl = 0.0524, train err = 0.0059, test err = 0.3043\n",
            "Iteration 95, time = 9s, loss_lab = 4.9551, loss_unl = 0.0555, train err = 0.0067, test err = 0.2978\n",
            "Iteration 96, time = 9s, loss_lab = 4.9507, loss_unl = 0.0528, train err = 0.0058, test err = 0.2939\n",
            "Iteration 97, time = 9s, loss_lab = 4.9519, loss_unl = 0.0566, train err = 0.0068, test err = 0.2915\n",
            "Iteration 98, time = 9s, loss_lab = 4.9538, loss_unl = 0.0573, train err = 0.0057, test err = 0.3000\n",
            "Iteration 99, time = 9s, loss_lab = 4.9494, loss_unl = 0.0524, train err = 0.0052, test err = 0.2991\n",
            "Iteration 100, time = 9s, loss_lab = 4.9552, loss_unl = 0.0568, train err = 0.0059, test err = 0.3074\n",
            "Iteration 101, time = 9s, loss_lab = 4.9518, loss_unl = 0.0541, train err = 0.0055, test err = 0.3001\n",
            "Iteration 102, time = 9s, loss_lab = 4.9533, loss_unl = 0.0557, train err = 0.0058, test err = 0.3014\n",
            "Iteration 103, time = 9s, loss_lab = 4.9512, loss_unl = 0.0511, train err = 0.0054, test err = 0.2955\n",
            "Iteration 104, time = 9s, loss_lab = 4.9596, loss_unl = 0.0555, train err = 0.0061, test err = 0.2906\n",
            "Iteration 105, time = 9s, loss_lab = 4.9490, loss_unl = 0.0549, train err = 0.0056, test err = 0.3011\n",
            "Iteration 106, time = 9s, loss_lab = 4.9530, loss_unl = 0.0579, train err = 0.0053, test err = 0.2990\n",
            "Iteration 107, time = 9s, loss_lab = 4.9535, loss_unl = 0.0528, train err = 0.0063, test err = 0.3014\n",
            "Iteration 108, time = 9s, loss_lab = 4.9642, loss_unl = 0.0603, train err = 0.0062, test err = 0.2980\n",
            "Iteration 109, time = 9s, loss_lab = 4.9491, loss_unl = 0.0539, train err = 0.0052, test err = 0.2929\n",
            "Iteration 110, time = 9s, loss_lab = 4.9460, loss_unl = 0.0486, train err = 0.0062, test err = 0.2955\n",
            "Iteration 111, time = 9s, loss_lab = 4.9493, loss_unl = 0.0492, train err = 0.0058, test err = 0.2880\n",
            "Iteration 112, time = 9s, loss_lab = 4.9490, loss_unl = 0.0510, train err = 0.0057, test err = 0.2955\n",
            "Iteration 113, time = 9s, loss_lab = 4.9538, loss_unl = 0.0581, train err = 0.0055, test err = 0.2956\n",
            "Iteration 114, time = 9s, loss_lab = 4.9512, loss_unl = 0.0542, train err = 0.0056, test err = 0.2937\n",
            "Iteration 115, time = 9s, loss_lab = 4.9467, loss_unl = 0.0547, train err = 0.0059, test err = 0.2921\n",
            "Iteration 116, time = 9s, loss_lab = 4.9489, loss_unl = 0.0559, train err = 0.0051, test err = 0.2998\n",
            "Iteration 117, time = 9s, loss_lab = 4.9497, loss_unl = 0.0543, train err = 0.0054, test err = 0.2966\n",
            "Iteration 118, time = 9s, loss_lab = 4.9488, loss_unl = 0.0561, train err = 0.0054, test err = 0.3045\n",
            "Iteration 119, time = 9s, loss_lab = 4.9520, loss_unl = 0.0587, train err = 0.0058, test err = 0.3107\n",
            "Iteration 120, time = 9s, loss_lab = 4.9479, loss_unl = 0.0534, train err = 0.0054, test err = 0.2979\n",
            "Iteration 121, time = 9s, loss_lab = 4.9504, loss_unl = 0.0572, train err = 0.0051, test err = 0.3068\n",
            "Iteration 122, time = 9s, loss_lab = 4.9508, loss_unl = 0.0536, train err = 0.0061, test err = 0.2995\n",
            "Iteration 123, time = 9s, loss_lab = 4.9468, loss_unl = 0.0543, train err = 0.0053, test err = 0.2963\n",
            "Iteration 124, time = 9s, loss_lab = 4.9458, loss_unl = 0.0521, train err = 0.0056, test err = 0.2852\n",
            "Iteration 125, time = 9s, loss_lab = 4.9533, loss_unl = 0.0539, train err = 0.0058, test err = 0.2950\n",
            "Iteration 126, time = 9s, loss_lab = 4.9477, loss_unl = 0.0548, train err = 0.0053, test err = 0.2935\n",
            "Iteration 127, time = 9s, loss_lab = 4.9472, loss_unl = 0.0538, train err = 0.0049, test err = 0.3037\n",
            "Iteration 128, time = 9s, loss_lab = 4.9497, loss_unl = 0.0533, train err = 0.0055, test err = 0.2908\n",
            "Iteration 129, time = 9s, loss_lab = 4.9423, loss_unl = 0.0522, train err = 0.0055, test err = 0.2902\n",
            "Iteration 130, time = 9s, loss_lab = 8.0506, loss_unl = 0.8914, train err = 0.0816, test err = 0.5600\n",
            "Iteration 131, time = 9s, loss_lab = 5.0271, loss_unl = 0.0589, train err = 0.0138, test err = 0.2962\n",
            "Iteration 132, time = 9s, loss_lab = 4.9429, loss_unl = 0.0497, train err = 0.0051, test err = 0.3050\n",
            "Iteration 133, time = 9s, loss_lab = 4.9342, loss_unl = 0.0516, train err = 0.0043, test err = 0.2979\n",
            "Iteration 134, time = 9s, loss_lab = 4.9357, loss_unl = 0.0498, train err = 0.0046, test err = 0.2921\n",
            "Iteration 135, time = 9s, loss_lab = 4.9367, loss_unl = 0.0465, train err = 0.0050, test err = 0.3002\n",
            "Iteration 136, time = 9s, loss_lab = 4.9361, loss_unl = 0.0553, train err = 0.0052, test err = 0.2945\n",
            "Iteration 137, time = 9s, loss_lab = 4.9346, loss_unl = 0.0541, train err = 0.0046, test err = 0.2925\n",
            "Iteration 138, time = 9s, loss_lab = 4.9375, loss_unl = 0.0530, train err = 0.0049, test err = 0.3042\n",
            "Iteration 139, time = 9s, loss_lab = 4.9373, loss_unl = 0.0523, train err = 0.0052, test err = 0.2987\n",
            "Iteration 140, time = 9s, loss_lab = 4.9342, loss_unl = 0.0526, train err = 0.0049, test err = 0.2906\n",
            "Iteration 141, time = 9s, loss_lab = 4.9406, loss_unl = 0.0572, train err = 0.0049, test err = 0.2992\n",
            "Iteration 142, time = 9s, loss_lab = 4.9408, loss_unl = 0.0567, train err = 0.0052, test err = 0.2981\n",
            "Iteration 143, time = 9s, loss_lab = 4.9377, loss_unl = 0.0612, train err = 0.0046, test err = 0.2973\n",
            "Iteration 144, time = 9s, loss_lab = 4.9358, loss_unl = 0.0524, train err = 0.0044, test err = 0.2970\n",
            "Iteration 145, time = 9s, loss_lab = 4.9376, loss_unl = 0.0540, train err = 0.0051, test err = 0.3004\n",
            "Iteration 146, time = 9s, loss_lab = 4.9399, loss_unl = 0.0542, train err = 0.0046, test err = 0.2972\n",
            "Iteration 147, time = 9s, loss_lab = 4.9470, loss_unl = 0.0562, train err = 0.0055, test err = 0.2965\n",
            "Iteration 148, time = 9s, loss_lab = 4.9438, loss_unl = 0.0561, train err = 0.0053, test err = 0.2913\n",
            "Iteration 149, time = 9s, loss_lab = 4.9458, loss_unl = 0.0547, train err = 0.0050, test err = 0.2950\n",
            "Iteration 150, time = 9s, loss_lab = 4.9461, loss_unl = 0.0524, train err = 0.0057, test err = 0.2989\n",
            "Iteration 151, time = 9s, loss_lab = 4.9488, loss_unl = 0.0529, train err = 0.0061, test err = 0.2880\n",
            "Iteration 152, time = 9s, loss_lab = 4.9511, loss_unl = 0.0559, train err = 0.0058, test err = 0.3020\n",
            "Iteration 153, time = 9s, loss_lab = 4.9413, loss_unl = 0.0522, train err = 0.0057, test err = 0.2959\n",
            "Iteration 154, time = 9s, loss_lab = 4.9518, loss_unl = 0.0575, train err = 0.0055, test err = 0.2938\n",
            "Iteration 155, time = 9s, loss_lab = 4.9457, loss_unl = 0.0563, train err = 0.0053, test err = 0.2857\n",
            "Iteration 156, time = 9s, loss_lab = 4.9393, loss_unl = 0.0542, train err = 0.0051, test err = 0.2992\n",
            "Iteration 157, time = 9s, loss_lab = 4.9387, loss_unl = 0.0559, train err = 0.0048, test err = 0.2953\n",
            "Iteration 158, time = 9s, loss_lab = 7.1789, loss_unl = 0.5019, train err = 0.0847, test err = 0.3068\n",
            "Iteration 159, time = 9s, loss_lab = 4.9529, loss_unl = 0.0624, train err = 0.0059, test err = 0.2969\n",
            "Iteration 160, time = 9s, loss_lab = 4.9380, loss_unl = 0.0535, train err = 0.0048, test err = 0.2994\n",
            "Iteration 161, time = 9s, loss_lab = 4.9308, loss_unl = 0.0568, train err = 0.0043, test err = 0.3010\n",
            "Iteration 162, time = 9s, loss_lab = 4.9284, loss_unl = 0.0533, train err = 0.0049, test err = 0.2901\n",
            "Iteration 163, time = 9s, loss_lab = 4.9356, loss_unl = 0.0523, train err = 0.0050, test err = 0.2915\n",
            "Iteration 164, time = 9s, loss_lab = 4.9349, loss_unl = 0.0526, train err = 0.0055, test err = 0.3010\n",
            "Iteration 165, time = 9s, loss_lab = 4.9369, loss_unl = 0.0536, train err = 0.0053, test err = 0.2972\n",
            "Iteration 166, time = 9s, loss_lab = 4.9349, loss_unl = 0.0535, train err = 0.0047, test err = 0.2894\n",
            "Iteration 167, time = 9s, loss_lab = 4.9389, loss_unl = 0.0522, train err = 0.0056, test err = 0.2945\n",
            "Iteration 168, time = 9s, loss_lab = 4.9375, loss_unl = 0.0549, train err = 0.0044, test err = 0.2991\n",
            "Iteration 169, time = 9s, loss_lab = 4.9357, loss_unl = 0.0537, train err = 0.0046, test err = 0.2998\n",
            "Iteration 170, time = 9s, loss_lab = 4.9345, loss_unl = 0.0549, train err = 0.0053, test err = 0.2975\n",
            "Iteration 171, time = 9s, loss_lab = 4.9392, loss_unl = 0.0534, train err = 0.0055, test err = 0.2908\n",
            "Iteration 172, time = 9s, loss_lab = 4.9414, loss_unl = 0.0566, train err = 0.0054, test err = 0.2950\n",
            "Iteration 173, time = 9s, loss_lab = 4.9434, loss_unl = 0.0592, train err = 0.0054, test err = 0.2969\n",
            "Iteration 174, time = 9s, loss_lab = 4.9475, loss_unl = 0.0575, train err = 0.0052, test err = 0.2942\n",
            "Iteration 175, time = 9s, loss_lab = 4.9386, loss_unl = 0.0524, train err = 0.0052, test err = 0.2942\n",
            "Iteration 176, time = 9s, loss_lab = 4.9447, loss_unl = 0.0555, train err = 0.0052, test err = 0.3110\n",
            "Iteration 177, time = 9s, loss_lab = 4.9372, loss_unl = 0.0539, train err = 0.0051, test err = 0.2989\n",
            "Iteration 178, time = 9s, loss_lab = 4.9387, loss_unl = 0.0503, train err = 0.0051, test err = 0.3005\n",
            "Iteration 179, time = 9s, loss_lab = 4.9439, loss_unl = 0.0556, train err = 0.0053, test err = 0.2942\n",
            "Iteration 180, time = 9s, loss_lab = 4.9455, loss_unl = 0.0548, train err = 0.0052, test err = 0.2963\n",
            "Iteration 181, time = 9s, loss_lab = 4.9476, loss_unl = 0.0527, train err = 0.0059, test err = 0.3083\n",
            "Iteration 182, time = 9s, loss_lab = 4.9390, loss_unl = 0.0540, train err = 0.0053, test err = 0.3046\n",
            "Iteration 183, time = 9s, loss_lab = 4.9357, loss_unl = 0.0524, train err = 0.0052, test err = 0.2993\n",
            "Iteration 184, time = 9s, loss_lab = 4.9473, loss_unl = 0.0556, train err = 0.0062, test err = 0.2969\n",
            "Iteration 185, time = 9s, loss_lab = 4.9408, loss_unl = 0.0547, train err = 0.0058, test err = 0.3034\n",
            "Iteration 186, time = 9s, loss_lab = 4.9520, loss_unl = 0.0535, train err = 0.0060, test err = 0.2939\n",
            "Iteration 187, time = 9s, loss_lab = 4.9451, loss_unl = 0.0534, train err = 0.0055, test err = 0.2993\n",
            "Iteration 188, time = 9s, loss_lab = 4.9349, loss_unl = 0.0503, train err = 0.0051, test err = 0.2984\n",
            "Iteration 189, time = 9s, loss_lab = 4.9382, loss_unl = 0.0543, train err = 0.0053, test err = 0.3056\n",
            "Iteration 190, time = 9s, loss_lab = 4.9420, loss_unl = 0.0550, train err = 0.0055, test err = 0.2909\n",
            "Iteration 191, time = 9s, loss_lab = 4.9379, loss_unl = 0.0533, train err = 0.0050, test err = 0.3056\n",
            "Iteration 192, time = 9s, loss_lab = 4.9437, loss_unl = 0.0572, train err = 0.0048, test err = 0.2911\n",
            "Iteration 193, time = 9s, loss_lab = 4.9391, loss_unl = 0.0527, train err = 0.0052, test err = 0.2907\n",
            "Iteration 194, time = 9s, loss_lab = 4.9363, loss_unl = 0.0529, train err = 0.0049, test err = 0.2925\n",
            "Iteration 195, time = 9s, loss_lab = 4.9381, loss_unl = 0.0502, train err = 0.0050, test err = 0.2961\n",
            "Iteration 196, time = 9s, loss_lab = 4.9434, loss_unl = 0.0555, train err = 0.0056, test err = 0.3030\n",
            "Iteration 197, time = 9s, loss_lab = 4.9367, loss_unl = 0.0514, train err = 0.0055, test err = 0.2947\n",
            "Iteration 198, time = 9s, loss_lab = 4.9320, loss_unl = 0.0509, train err = 0.0052, test err = 0.2967\n",
            "Iteration 199, time = 9s, loss_lab = 4.9354, loss_unl = 0.0537, train err = 0.0043, test err = 0.3035\n",
            "Iteration 200, time = 9s, loss_lab = 4.9431, loss_unl = 0.0520, train err = 0.0049, test err = 0.3069\n",
            "Iteration 201, time = 9s, loss_lab = 4.9323, loss_unl = 0.0554, train err = 0.0048, test err = 0.3054\n",
            "Iteration 202, time = 9s, loss_lab = 4.9408, loss_unl = 0.0548, train err = 0.0055, test err = 0.2919\n",
            "Iteration 203, time = 9s, loss_lab = 4.9323, loss_unl = 0.0566, train err = 0.0047, test err = 0.2926\n",
            "Iteration 204, time = 9s, loss_lab = 4.9400, loss_unl = 0.0547, train err = 0.0050, test err = 0.2952\n",
            "Iteration 205, time = 9s, loss_lab = 4.9338, loss_unl = 0.0519, train err = 0.0049, test err = 0.2928\n",
            "Iteration 206, time = 9s, loss_lab = 4.9355, loss_unl = 0.0516, train err = 0.0053, test err = 0.2878\n",
            "Iteration 207, time = 9s, loss_lab = 4.9399, loss_unl = 0.0503, train err = 0.0054, test err = 0.2988\n",
            "Iteration 208, time = 9s, loss_lab = 4.9420, loss_unl = 0.0559, train err = 0.0056, test err = 0.2927\n",
            "Iteration 209, time = 9s, loss_lab = 4.9340, loss_unl = 0.0492, train err = 0.0053, test err = 0.2929\n",
            "Iteration 210, time = 9s, loss_lab = 4.9357, loss_unl = 0.0528, train err = 0.0050, test err = 0.2899\n",
            "Iteration 211, time = 9s, loss_lab = 4.9352, loss_unl = 0.0512, train err = 0.0046, test err = 0.2972\n",
            "Iteration 212, time = 9s, loss_lab = 4.9359, loss_unl = 0.0509, train err = 0.0051, test err = 0.2969\n",
            "Iteration 213, time = 9s, loss_lab = 4.9365, loss_unl = 0.0467, train err = 0.0054, test err = 0.2964\n",
            "Iteration 214, time = 9s, loss_lab = 4.9377, loss_unl = 0.0510, train err = 0.0053, test err = 0.2855\n",
            "Iteration 215, time = 9s, loss_lab = 4.9456, loss_unl = 0.0560, train err = 0.0060, test err = 0.2955\n",
            "Iteration 216, time = 9s, loss_lab = 4.9382, loss_unl = 0.0533, train err = 0.0051, test err = 0.2988\n",
            "Iteration 217, time = 9s, loss_lab = 4.9323, loss_unl = 0.0516, train err = 0.0047, test err = 0.2962\n",
            "Iteration 218, time = 9s, loss_lab = 10.7471, loss_unl = 1.4578, train err = 0.1267, test err = 0.3014\n",
            "Iteration 219, time = 9s, loss_lab = 4.9491, loss_unl = 0.0580, train err = 0.0065, test err = 0.2959\n",
            "Iteration 220, time = 9s, loss_lab = 4.9361, loss_unl = 0.0505, train err = 0.0058, test err = 0.3008\n",
            "Iteration 221, time = 9s, loss_lab = 4.9337, loss_unl = 0.0491, train err = 0.0052, test err = 0.2957\n",
            "Iteration 222, time = 9s, loss_lab = 4.9356, loss_unl = 0.0583, train err = 0.0048, test err = 0.2922\n",
            "Iteration 223, time = 9s, loss_lab = 4.9348, loss_unl = 0.0525, train err = 0.0050, test err = 0.3000\n",
            "Iteration 224, time = 9s, loss_lab = 4.9350, loss_unl = 0.0470, train err = 0.0059, test err = 0.2972\n",
            "Iteration 225, time = 9s, loss_lab = 4.9346, loss_unl = 0.0510, train err = 0.0054, test err = 0.2956\n",
            "Iteration 226, time = 9s, loss_lab = 4.9393, loss_unl = 0.0508, train err = 0.0053, test err = 0.3040\n",
            "Iteration 227, time = 9s, loss_lab = 4.9364, loss_unl = 0.0519, train err = 0.0048, test err = 0.2968\n",
            "Iteration 228, time = 9s, loss_lab = 4.9350, loss_unl = 0.0505, train err = 0.0050, test err = 0.3034\n",
            "Iteration 229, time = 9s, loss_lab = 4.9334, loss_unl = 0.0507, train err = 0.0051, test err = 0.2863\n",
            "Iteration 230, time = 9s, loss_lab = 4.9328, loss_unl = 0.0510, train err = 0.0048, test err = 0.2981\n",
            "Iteration 231, time = 9s, loss_lab = 4.9466, loss_unl = 0.0607, train err = 0.0056, test err = 0.2953\n",
            "Iteration 232, time = 9s, loss_lab = 4.9360, loss_unl = 0.0513, train err = 0.0057, test err = 0.2998\n",
            "Iteration 233, time = 9s, loss_lab = 4.9331, loss_unl = 0.0570, train err = 0.0048, test err = 0.2898\n",
            "Iteration 234, time = 9s, loss_lab = 4.9299, loss_unl = 0.0525, train err = 0.0048, test err = 0.2995\n",
            "Iteration 235, time = 9s, loss_lab = 4.9328, loss_unl = 0.0533, train err = 0.0049, test err = 0.2973\n",
            "Iteration 236, time = 9s, loss_lab = 4.9400, loss_unl = 0.0580, train err = 0.0050, test err = 0.2923\n",
            "Iteration 237, time = 9s, loss_lab = 4.9372, loss_unl = 0.0514, train err = 0.0049, test err = 0.3039\n",
            "Iteration 238, time = 9s, loss_lab = 4.9335, loss_unl = 0.0550, train err = 0.0050, test err = 0.2968\n",
            "Iteration 239, time = 9s, loss_lab = 7.7453, loss_unl = 0.7728, train err = 0.0757, test err = 0.3025\n",
            "Iteration 240, time = 9s, loss_lab = 4.9468, loss_unl = 0.0559, train err = 0.0051, test err = 0.2995\n",
            "Iteration 241, time = 9s, loss_lab = 4.9354, loss_unl = 0.0514, train err = 0.0055, test err = 0.2949\n",
            "Iteration 242, time = 9s, loss_lab = 4.9318, loss_unl = 0.0518, train err = 0.0050, test err = 0.2989\n",
            "Iteration 243, time = 9s, loss_lab = 4.9347, loss_unl = 0.0534, train err = 0.0046, test err = 0.2976\n",
            "Iteration 244, time = 9s, loss_lab = 4.9361, loss_unl = 0.0549, train err = 0.0050, test err = 0.3063\n",
            "Iteration 245, time = 9s, loss_lab = 4.9256, loss_unl = 0.0491, train err = 0.0052, test err = 0.2975\n",
            "Iteration 246, time = 9s, loss_lab = 4.9306, loss_unl = 0.0524, train err = 0.0056, test err = 0.2929\n",
            "Iteration 247, time = 9s, loss_lab = 4.9302, loss_unl = 0.0500, train err = 0.0053, test err = 0.2972\n",
            "Iteration 248, time = 9s, loss_lab = 4.9268, loss_unl = 0.0514, train err = 0.0046, test err = 0.2897\n",
            "Iteration 249, time = 9s, loss_lab = 4.9338, loss_unl = 0.0506, train err = 0.0051, test err = 0.2887\n",
            "Iteration 250, time = 9s, loss_lab = 4.9291, loss_unl = 0.0530, train err = 0.0050, test err = 0.2869\n",
            "Iteration 251, time = 9s, loss_lab = 4.9310, loss_unl = 0.0522, train err = 0.0048, test err = 0.2888\n",
            "Iteration 252, time = 9s, loss_lab = 4.9309, loss_unl = 0.0536, train err = 0.0050, test err = 0.2944\n",
            "Iteration 253, time = 9s, loss_lab = 4.9348, loss_unl = 0.0541, train err = 0.0049, test err = 0.2921\n",
            "Iteration 254, time = 9s, loss_lab = 4.9340, loss_unl = 0.0536, train err = 0.0053, test err = 0.2940\n",
            "Iteration 255, time = 9s, loss_lab = 4.9317, loss_unl = 0.0537, train err = 0.0055, test err = 0.3095\n",
            "Iteration 256, time = 9s, loss_lab = 4.9345, loss_unl = 0.0522, train err = 0.0047, test err = 0.2946\n",
            "Iteration 257, time = 9s, loss_lab = 4.9366, loss_unl = 0.0528, train err = 0.0051, test err = 0.2872\n",
            "Iteration 258, time = 9s, loss_lab = 4.9348, loss_unl = 0.0514, train err = 0.0046, test err = 0.2889\n",
            "Iteration 259, time = 9s, loss_lab = 4.9344, loss_unl = 0.0492, train err = 0.0054, test err = 0.3057\n",
            "Iteration 260, time = 9s, loss_lab = 4.9407, loss_unl = 0.0530, train err = 0.0057, test err = 0.3046\n",
            "Iteration 261, time = 9s, loss_lab = 4.9354, loss_unl = 0.0535, train err = 0.0046, test err = 0.2997\n",
            "Iteration 262, time = 9s, loss_lab = 4.9354, loss_unl = 0.0489, train err = 0.0051, test err = 0.2990\n",
            "Iteration 263, time = 9s, loss_lab = 4.9306, loss_unl = 0.0505, train err = 0.0049, test err = 0.2953\n",
            "Iteration 264, time = 9s, loss_lab = 4.9308, loss_unl = 0.0531, train err = 0.0045, test err = 0.2931\n",
            "Iteration 265, time = 9s, loss_lab = 4.9363, loss_unl = 0.0540, train err = 0.0046, test err = 0.3094\n",
            "Iteration 266, time = 9s, loss_lab = 4.9306, loss_unl = 0.0536, train err = 0.0046, test err = 0.2900\n",
            "Iteration 267, time = 9s, loss_lab = 4.9339, loss_unl = 0.0501, train err = 0.0059, test err = 0.3095\n",
            "Iteration 268, time = 9s, loss_lab = 4.9350, loss_unl = 0.0522, train err = 0.0049, test err = 0.2975\n",
            "Iteration 269, time = 9s, loss_lab = 4.9304, loss_unl = 0.0521, train err = 0.0047, test err = 0.2903\n",
            "Iteration 270, time = 9s, loss_lab = 4.9381, loss_unl = 0.0508, train err = 0.0053, test err = 0.2913\n",
            "Iteration 271, time = 9s, loss_lab = 4.9320, loss_unl = 0.0501, train err = 0.0046, test err = 0.2917\n",
            "Iteration 272, time = 9s, loss_lab = 4.9398, loss_unl = 0.0550, train err = 0.0048, test err = 0.2959\n",
            "Iteration 273, time = 9s, loss_lab = 4.9278, loss_unl = 0.0503, train err = 0.0047, test err = 0.3013\n",
            "Iteration 274, time = 9s, loss_lab = 4.9485, loss_unl = 0.0513, train err = 0.0067, test err = 0.2953\n",
            "Iteration 275, time = 9s, loss_lab = 4.9304, loss_unl = 0.0460, train err = 0.0056, test err = 0.2944\n",
            "Iteration 276, time = 9s, loss_lab = 4.9295, loss_unl = 0.0521, train err = 0.0051, test err = 0.3082\n",
            "Iteration 277, time = 9s, loss_lab = 4.9343, loss_unl = 0.0514, train err = 0.0048, test err = 0.3003\n",
            "Iteration 278, time = 9s, loss_lab = 4.9357, loss_unl = 0.0497, train err = 0.0051, test err = 0.2976\n",
            "Iteration 279, time = 9s, loss_lab = 4.9331, loss_unl = 0.0543, train err = 0.0052, test err = 0.2967\n",
            "Iteration 280, time = 9s, loss_lab = 4.9321, loss_unl = 0.0520, train err = 0.0055, test err = 0.2978\n",
            "Iteration 281, time = 9s, loss_lab = 4.9304, loss_unl = 0.0519, train err = 0.0050, test err = 0.2965\n",
            "Iteration 282, time = 9s, loss_lab = 4.9372, loss_unl = 0.0541, train err = 0.0050, test err = 0.3002\n",
            "Iteration 283, time = 9s, loss_lab = 4.9316, loss_unl = 0.0519, train err = 0.0053, test err = 0.2952\n",
            "Iteration 284, time = 9s, loss_lab = 4.9291, loss_unl = 0.0511, train err = 0.0047, test err = 0.2924\n",
            "Iteration 285, time = 9s, loss_lab = 4.9355, loss_unl = 0.0548, train err = 0.0051, test err = 0.2894\n",
            "Iteration 286, time = 9s, loss_lab = 4.9368, loss_unl = 0.0549, train err = 0.0056, test err = 0.2899\n",
            "Iteration 287, time = 9s, loss_lab = 4.9330, loss_unl = 0.0518, train err = 0.0053, test err = 0.3013\n",
            "Iteration 288, time = 9s, loss_lab = 4.9346, loss_unl = 0.0490, train err = 0.0051, test err = 0.2917\n",
            "Iteration 289, time = 9s, loss_lab = 4.9255, loss_unl = 0.0499, train err = 0.0046, test err = 0.2981\n",
            "Iteration 290, time = 9s, loss_lab = 4.9351, loss_unl = 0.0548, train err = 0.0050, test err = 0.2941\n",
            "Iteration 291, time = 9s, loss_lab = 4.9326, loss_unl = 0.0521, train err = 0.0051, test err = 0.2936\n",
            "Iteration 292, time = 9s, loss_lab = 4.9321, loss_unl = 0.0533, train err = 0.0050, test err = 0.2906\n",
            "Iteration 293, time = 9s, loss_lab = 4.9407, loss_unl = 0.0505, train err = 0.0056, test err = 0.3044\n",
            "Iteration 294, time = 9s, loss_lab = 4.9310, loss_unl = 0.0496, train err = 0.0053, test err = 0.2961\n",
            "Iteration 295, time = 9s, loss_lab = 4.9304, loss_unl = 0.0507, train err = 0.0054, test err = 0.2985\n",
            "Iteration 296, time = 9s, loss_lab = 4.9315, loss_unl = 0.0546, train err = 0.0047, test err = 0.2937\n",
            "Iteration 297, time = 9s, loss_lab = 4.9324, loss_unl = 0.0484, train err = 0.0058, test err = 0.3016\n",
            "Iteration 298, time = 9s, loss_lab = 4.9275, loss_unl = 0.0510, train err = 0.0044, test err = 0.3076\n",
            "Iteration 299, time = 9s, loss_lab = 4.9313, loss_unl = 0.0513, train err = 0.0050, test err = 0.2954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hyT_SzN7xvMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K3xKgKAsjwCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images(Generator, save2file=False, samples=25, step=0):\n",
        "    filename = \"./images/mnist_%d.png\" % step\n",
        "    #noise = np.random.normal(0, 1, (samples, 100))\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "    images = Generator.predict(noise)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        #image = images[i, :, :, :]\n",
        "        image = images[i, :]\n",
        "        image = np.reshape(image, [28, 28])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save2file:\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "    else:\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRTW3QgozC5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "17999831-e340-44c5-c96d-b9fe43edc19a"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8db4561240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8db4561240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "44wU0x17zT_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajoM0MlTvxdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "40cf3be4-78ff-49ea-c4a5-a9e3f0a07453"
      },
      "cell_type": "code",
      "source": [
        "plot_images(generator)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8db1aca748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Kz18f09Oxn8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}