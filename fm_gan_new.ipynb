{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, BatchNormalization, Dense, GaussianNoise\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        # generator\n",
    "        self.input = Input([100])\n",
    "        layer = Dense(500, activation=K.softplus)(self.input)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dense(500, activation=K.softplus)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        self.output = Dense(28 ** 2, activation=K.sigmoid, kernel_regularizer=regularizers.l2())(layer)\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        # discriminator\n",
    "        self.input = Input([28 ** 2])\n",
    "        layer = GaussianNoise(stddev=0.3)(self.input)\n",
    "        layer = Dense(1000)(layer)\n",
    "        layer = GaussianNoise(stddev=0.5)(layer)\n",
    "        layer = Dense(500)(layer)\n",
    "        layer = GaussianNoise(stddev=0.5)(layer)\n",
    "        layer = Dense(250)(layer)\n",
    "        layer = GaussianNoise(stddev=0.5)(layer)\n",
    "        layer = Dense(250)(layer)\n",
    "        layer = GaussianNoise(stddev=0.5)(layer)\n",
    "        layer = Dense(250)(layer)\n",
    "        self.feature = Model(inputs=self.input, outputs=layer)\n",
    "        layer = GaussianNoise(stddev=0.5)(layer)\n",
    "        self.output = Dense(10)(layer)\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_gen(batch_size, z_dim):\n",
    "    noise = np.zeros((batch_size, z_dim), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        noise[i, :] = np.random.uniform(0, 1, z_dim)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--seed', type=int, default=1)\n",
    "#parser.add_argument('--seed_data', type=int, default=1)\n",
    "#parser.add_argument('--unlabeled_weight', type=float, default=1.)\n",
    "#parser.add_argument('--batch_size', type=int, default=100)\n",
    "#parser.add_argument('--count', type=int, default=10)\n",
    "#args = parser.parse_args()\n",
    "#print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "seed_data = 1\n",
    "unlabeled_weight = 1\n",
    "batch_size = 100\n",
    "count =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "data = np.load('mnist.npz')\n",
    "trainx = np.concatenate([data['x_train'], data['x_valid']], axis=0)\n",
    "trainx_unl = trainx.copy()\n",
    "trainx_unl2 = trainx.copy()\n",
    "trainy = np.concatenate([data['y_train'], data['y_valid']]).astype(np.int32)\n",
    "#nr_batches_train = int(trainx.shape[0] / args.batch_size)\n",
    "nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "testx = data['x_test']\n",
    "testy = data['y_test'].astype(np.int32)\n",
    "testy = np.reshape(testy, [testy.shape[0], 1])\n",
    "#nr_batches_test = int(testx.shape[0] / args.batch_size)\n",
    "nr_batches_test = int(testx.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 784 but corresponding boolean dimension is 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5ff4b9e4e2d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 784 but corresponding boolean dimension is 1"
     ]
    }
   ],
   "source": [
    "# select labeled data\n",
    "rng = np.random.RandomState(seed)\n",
    "data_rng = np.random.RandomState(seed_data)\n",
    "inds = data_rng.permutation(trainx.shape[0])\n",
    "trainx = trainx[inds]\n",
    "trainy = trainy[inds]\n",
    "txs = []\n",
    "tys = []\n",
    "for j in range(10):\n",
    "    txs.append(trainx[trainy == j][:count])\n",
    "    tys.append(trainy[trainy == j][:count])\n",
    "txs = np.concatenate(txs, axis=0)\n",
    "tys = np.concatenate(tys, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tensorflow and keras\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "discriminator_model = Discriminator()\n",
    "discriminator = discriminator_model.model\n",
    "discriminator_feature = discriminator_model.feature\n",
    "generator_model = Generator()\n",
    "generator = generator_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function computation\n",
    "x_label = Input([28 ** 2])\n",
    "x_unlabel = Input([28 ** 2])\n",
    "labels = Input([1], dtype=tf.int32)\n",
    "noise = Input([100])\n",
    "fake_image = generator(noise)\n",
    "output_before_softmax_label = discriminator(x_label)\n",
    "output_before_softmax_unlabel = discriminator(x_unlabel)\n",
    "output_before_softmax_fake = discriminator(fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_exp_label = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_label))\n",
    "z_exp_unlabel = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_unlabel))\n",
    "z_exp_fake = tf.reduce_mean(tf.reduce_logsumexp(output_before_softmax_fake))\n",
    "index_flattened = tf.range(0, batch_size) * output_before_softmax_label.shape[1] + tf.reshape(labels, [batch_size])\n",
    "l_label = tf.gather(tf.reshape(output_before_softmax_label, [-1]), index_flattened)\n",
    "l_unlabel = tf.reduce_logsumexp(output_before_softmax_unlabel)\n",
    "loss_label = -tf.reduce_mean(l_label) + tf.reduce_mean(z_exp_label)\n",
    "loss_unlabel = -0.5 * tf.reduce_mean(l_unlabel) + 0.5 * tf.reduce_mean(tf.nn.softplus(l_unlabel)) + \\\n",
    "               0.5 * tf.reduce_mean(tf.nn.softplus(tf.reduce_logsumexp(output_before_softmax_fake)))\n",
    "loss_discriminator = tf.add(loss_label, tf.multiply(loss_unlabel, unlabeled_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generated = tf.reduce_mean(discriminator_feature(fake_image), axis=0)\n",
    "feature_real = tf.reduce_mean(discriminator_feature(x_unlabel), axis=0)\n",
    "loss_generator = tf.reduce_mean(tf.square(feature_generated - feature_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err = tf.reduce_mean(\n",
    "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))\n",
    "test_error = tf.reduce_mean(\n",
    "    tf.to_float(tf.not_equal(tf.argmax(output_before_softmax_label, axis=1), tf.cast(tf.reshape(labels, [batch_size]), tf.int64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# train settings\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "discriminator_gradients = discriminator_optimizer.compute_gradients(loss_discriminator, discriminator.trainable_weights)\n",
    "# discriminator_grads_and_vars = zip(discriminator_gradients, discriminator.trainable_weights)\n",
    "discriminator_train = discriminator_optimizer.apply_gradients(discriminator_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "generator_gradients = generator_optimizer.compute_gradients(loss_generator, generator.trainable_weights)\n",
    "# generator_grads_and_vars = zip(generator_gradients, generator.trainable_weights)\n",
    "generator_train = generator_optimizer.apply_gradients(generator_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d271cad310a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mx_unlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainx_unl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mnoise\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnoise_feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             })\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    begin = time.time()\n",
    "\n",
    "    # construct randomly permuted minibatches\n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    for t in range(int(trainx_unl.shape[0] / txs.shape[0])):\n",
    "        inds = rng.permutation(txs.shape[0])\n",
    "        trainx.append(txs[inds])\n",
    "        trainy.append(tys[inds])\n",
    "    trainx = np.concatenate(trainx, axis=0)\n",
    "    trainy = np.concatenate(trainy, axis=0)\n",
    "    trainy = np.reshape(trainy, [trainy.shape[0], 1])\n",
    "    trainx_unl = trainx_unl[rng.permutation(trainx_unl.shape[0])]\n",
    "    trainx_unl2 = trainx_unl2[rng.permutation(trainx_unl2.shape[0])]\n",
    "\n",
    "    # train\n",
    "    loss_label_record = 0.\n",
    "    loss_unlabel_record = 0.\n",
    "    train_err_record = 0.\n",
    "    for t in range(nr_batches_train):\n",
    "        noise_feed = noise_gen(batch_size, 100)\n",
    "        _, loss_label_this, loss_unlabel_this, train_err_this = sess.run(\n",
    "            [discriminator_train, loss_label, loss_unlabel, train_err], feed_dict={\n",
    "                x_label: trainx[t * batch_size:(t + 1) * batch_size],\n",
    "                x_unlabel: trainx_unl[t * batch_size:(t + 1) * batch_size],\n",
    "                labels: trainy[t * batch_size:(t + 1) * batch_size],\n",
    "                noise: noise_feed\n",
    "            })\n",
    "\n",
    "        loss_label_record += loss_label_this\n",
    "        loss_unlabel_record += loss_unlabel_this\n",
    "        train_err_record += train_err_this\n",
    "\n",
    "        noise_feed = noise_gen(batch_size, 100)\n",
    "        _, loss_generator_this = sess.run([generator_train, loss_generator], feed_dict={\n",
    "            noise: noise_feed,\n",
    "            x_unlabel: trainx_unl2[t * batch_size:(t + 1) * batch_size]\n",
    "        })\n",
    "    loss_label_record /= nr_batches_train\n",
    "    loss_unlabel_record /= nr_batches_train\n",
    "    train_err_record /= nr_batches_train\n",
    "\n",
    "    # test\n",
    "    test_err_record = 0.\n",
    "    for t in range(nr_batches_test):\n",
    "        test_err_this = sess.run(test_error, feed_dict={\n",
    "            x_label: testx[t * batch_size:(t + 1) * batch_size],\n",
    "            labels: testy[t * batch_size:(t + 1) * batch_size]\n",
    "        })\n",
    "        test_err_record += test_err_this\n",
    "    test_err_record /= nr_batches_test\n",
    "\n",
    "    # report\n",
    "    print(\"Iteration %d, time = %ds, loss_lab = %.4f, loss_unl = %.4f, train err = %.4f, test err = %.4f\" % (\n",
    "        epoch, time.time() - begin, loss_label_record, loss_unlabel_record, train_err_record, test_err_record))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
